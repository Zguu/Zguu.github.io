---
title: " [추천시스템] DRN : A Deep Reinforcement Learning Framework for News Recommendation"
tags: RecommenderSystem
---

# ABSTRACT
기존 추천 모델의 세가지 한계는 다음과 같습니다.
1) current reward에 해당하는 CTR과 같은 값들을 모델하는 데에만 치중하고 있다.
2) click 유무 외에 다른 종류의 유저 피드백 데이터들은 사용하고 있지 않다.
3) 이러한 추천 시스템들은 유저가 쉽게 지루할만한 컨텐츠를 추천한다.

저자는 위와 같은 기존 추천 시스템의 한계를 극복하기 위해서, Deep Q-Learning 기반의 추천 프레임워크를 제안하고 있습니다. 이는 future reward를 explicily 모덿링하며, 유저의 return 패턴 데이터를 클릭 데이터와 함께 활용하여 유저의 feedback 정보를 더욱 더 풍부하게 잡아낼 수 있습니다. 추가적으로, 유저에게 매력적인 새로운 컨텐츠를 추가하기 위한 효율적인 exploratoy 전략 또한 함께 제시됐습니다. 이러한 연구는 실제 상업 뉴스 데이터 (offline)와 상품 환경 (online)에서 테스트 됐습니다.

## INTRODUCTION

뉴스 추천 시스템에서 다른 추천 시스템들과는 다르게 추가로 고려야할 것들은 다음과 같습니다.

1. 뉴스 컨텐츠는 온라인에서 빠르게 outdated 됩니다. 처음으로 발행된 뉴스가 마지막 클릭까지 걸리는 시간은 4.1시간입니다. 즉, 4.1시간 이후에 대부분의 뉴스들은 유저들의 관심을 끌지 못합니다.
2. 시간이 지남에 따라 유저들의 관심사가 달라집니다. 예를 들어, 유저가 첫 몇주간 관심있는 토픽은 정치였는데, 이후에는 점차적으로 엔터테인먼트, 이후에는 기술 토픽으로 관심이 옮겨갈 수 있습니다. 그러므로 모델을 주기적으로 업데이트하는 것이 필수적입니다. 아래의 Figure 1에 10주간 유저의 관심사 변화가 잘 나타납니다.

![스크린샷 2020-12-09 오전 3.26.08](https://i.imgur.com/KDwC2bn.png)

"current reward에 해당하는 CTR과 같은 값들을 모델하는 데에만 치중하고 있다." 라는 한계를 극복하기 위해 추가로 고려할 수 있는 데이터들은 여러개가 있을 수 있는데, 그 중 하나는 장기적인 추천시스템의 퍼포먼스를 고려하는 것입니다. 다음의 예시에서 미래 고려에 대한 필요성에 대해 체감할 수 있습니다.

> Mike 라는 유저는 현재 태풍 특보와 농구 선수 코비 브라이언트 두 토픽의 주제 뉴스에 대해 동일한 관심사를 보이고 있습니다. 하지만 이 유저가 태풍 특보에 대해 미래에 더 이상의 관심사는 보이지 않을 것이고, 코비 브라이언트 뉴스에 대해서는 해당 뉴스를 본 이후에 농구라는 토픽으로 관심사가 옮겨갈 수 있습니다. 이러한 경우에 우리는 장기적으로 봤을 때, 태풍 특보 뉴스보다 코비브라이언트 뉴스를 추천하는 것이 더욱 효과적일 것임을 깨달을 수 있습니다.

"click 유무 외에 다른 종류의 유저 피드백 데이터들은 사용하고 있지 않다." 라는 한계를 극복하기 위해서는, CTR 뿐 아니라, 이 유저가 얼마나 빠르게 우리의 서비스에 재방문하는 지와 같은 지표를 활용해볼 수도 있습니다. 하지만, 이렇게 다양한 지표들을 활용하는 연구들은 아직 활발하지 않습니다.

"이러한 추천 시스템들은 유저가 쉽게 지루할만한 컨텐츠를 추천한다." 에 대한 문제를 극복하기 위해 연구들은 간단한 $\epsilon-greedy$ 혹은 $\text{Upper Confidencce Bound(UCB)}$ 와 같은 간단한 MAB 방법론을 활용해보기도 했습니다. 하지만 이 두가지 방법은 전혀 관련없는 토픽들을 추천하며 explore 한다는 위험성을 갖고 있습니다. 따라서 조금 더 효율적인 eplore를 위한 전략을 제시해야 합니다.

우리가 제시하는 Deep Reinforcement Learning 기반의 방법론은 위의 세가지 한계점을 극복할 수 있습니다.
