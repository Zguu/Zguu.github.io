---
title: " [머신러닝] GradientBoosting 알고리즘"
tags: MachineLearning
---

# Gradient Boosting 이란?
> 이 글은 https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/ 의 포스팅을 직역한 것입니다.

$\ $ 그래디언트 부스팅은 예측 모델을 만드는 데에 있어서 가장 강력한 기술 중 하나이다. 이 포스팅을 읽고난 후 다음 내용들을 알 수 있다.
- Learning Theory, AdaBoost 로부터 부스팅의 기원
- 어떻게 그래디언트 부스팅이 작동하는 지와, 손실 함수, 약한 학습기, additive model
- 다양한 regularization schemes을 활용하여 어떻게 기본 알고리즘의 퍼포먼스를 향상시키는지
이제 하나씩 알아보도록 하자!
## 부스팅(Boosting)의 기원
$\ $부스팅의 기본적인 아이디어는, 약한 학습기들을 반복적으로 활용하여 더욱 강력한 학습기를 만들 수 있을까에 대한 것이었다. 약한 학습기가 학습에 있어서 어려움과 misclassification을 찾아내면, 이 약점들에 대해 수정하고 학습해나가는 방향으로 더욱 강력한 학습기가 만들어진다.
## AdaBoost, 최초의 부스팅 알고리즘
$\ $실제 적용에서 첫 번째로 성공을 이뤄낸 부스팅 알고리즘은 Adaptive Boosting (AdaBoost)이다. AdaBoost 에서 약한 학습기는 하나의 split만을 갖고 있는 decision tree 이며, 짧게 decision stumps 라고 불려진다. AdaBoost는 관측 값들에 weighting을 주며 진행하며, 분류하기 더욱 어려운 개체들에는 weight를 더욱 주며, 반대로 분류가 잘 되는 개체들에는 weight를 덜 해주는 방식이다. 새로운 약한 학습기는 순차적으로 더해지는데, 이 과정에서 학습하기 더욱 어려운 패턴들에 집중하여 학습한다.
> This means that samples that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies these samples

예측은 약한 학습기들의 예측의 다수결 투표에 의해 만들어지며, 그들 각각의 정확도로 weight를 받아 투표가 이뤄진다. AdaBoost는 초창기에 binary classification에서 매우 성공적인 결과를 보여주었다.
## AdaBoosting을 넘어 조금 더 일반적인 부스팅 알고리즘으로..
$\ $AdaBoost 및 이와 관련된 알고리즘들이 통게적 개념 하에서 ARCing 알고리즘으로 개선됐다. Arcing은 Adaptive Reweighting and Combining 의 줄임말이다.
> Arcing is an acronym for Adaptive Reweighting and Combining. Each step in an arcing algorithm consists of a weighted minimization followed by a recomputation of [the classifiers] and [weighted input]

$\ $이러한 개념은 Gradient Boosting Machines 으로 추후에 더욱 발전됐으며, 줄여서 gradient boosting 또는 gradient tree boosting이라고 부른다. 통계학적 프레임 하에서는 이 부스팅을 수학적 최적화 문제로 보며, 이 문제에서 목적은 모델의 손실을 줄이는 것이다. 이러한 손실의 최소화는 Gradient Descent와 같은 과정을 통해 약한 학습기들을 더해나가면서 진행된다.<br>
$\ $이런 알고리즘의 종류는 stage-wise additive model로 불려지는데, 이것은 새로운 약한 학습기가 추가되고 기존의 약한 학습기는 얼려진(frozen) 상태로 더이상 변하지않고 남아있기 때문이다.
> Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added.

이러한 generalization은 미분 가능한 손실함수가 사용될 수 있는 것을 허용하며, binary classification 문제 뿐 아니라 회의, multi-class classification 과 다른 문제에도 적용을 가능하게 한다.

<내용 추가 예정>
