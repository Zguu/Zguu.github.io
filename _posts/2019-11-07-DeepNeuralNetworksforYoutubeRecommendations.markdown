---
title: " [논문 리뷰] Deep Neural Networks for YouTube Recommendations"
tags: PaperReview
---

# 유튜브 추천 시스템 논문
RecSys '16 September 논문이며 당시 학계에서 많은 주목을 받은 논문이다. 현재 유튜브의 추천 시스템은 이 시스템에서 훨씬 더 발전된 형태일 것이고 그에 대한 공부가 필요할 것이다. 하지만, 기존의 단순한 CF나 MF를 활용한 추천 시스템에서 더욱 나아가, DNN을 활용한 추천 시스템 연구들에 대한 공부를 시작하기에 가장 의미있을 것 같다는 생각을 했다.

---
## Introduction
 유튜브에서의 영상 추천 시스템은 크게 다음과 같은 세가지 난관이 있다.
 - Scale(규모의 문제) : 작은 데이터에서 잘 작용하던 기존의 추천시스템들이 유튜브와 같은 거대한 데이터에서 잘 작용하지 않는 경우가 많다. 유저 데이터와 영상 corpus 데이터가 다른 서비스와는 비교가 되지 않을 정도로 많다.
- Freshness(새로움에 대한 적응의 문제?) : 유튜브에는 1초당 a lot of hour(꽤 많은 시간)에 해당하는 영상들이 업로드된다. 추천 시스템은 새롭게 추가되는 영상 컨텐츠에도 잘 작용해야 한다. 이 문제는 exploitation/exploration의 관점으로 바라 볼 수도 있다.
- Noise(불완전한 데이터) : 유튜브에서 유저의 historical 행동은 예측하기가 매우 어려운데 이는 data 자체의 sparsity나 관측이 불가능한 외적 요인들에서 기인한다. 유저에 대한 explicit한 데이터들보다 implicit한 데이터들을 보통 얻는다.<br>
 이러한 트레이닝 데이터셋의 특징들에 robust(단단한) 모델을 만들어야 한다.
> 데이터 규모가 너무 크고, 유저들에 의한 새로운 데이터들이 계속 생성되며, 생성되는 데이터들 조차 dense하지 않고 sparse한 문제 등을 안고 있다.
Tensorflow 기반의 딥러닝으로 문제 해결하고자 했음. 기존의 MF 기반 방식들에 비해 해당 연구 시기(2016년 전후)에는 딥 러닝 기반의 추천 시스템에 대한 연구가 부족했음. 기존에 CF가 DNN이나 auto-encoder에 의해 모방된 연구들이 있긴 했다.

## System overview
후보군 선정, 랭킹 크게 두 부분으로 이뤄져 있다. 후보군 선정에서는 유튜브의 유저 활동 히스토리를 인풋으로 하고 매우 큰 corpus에서 수백개 정도의 작은 subset을 생성했다. 이 후보군들은 일반적으로 유저와 매우 높은 precision 연관이 있도록 돼있다. 후보군 선정 네트워크는 CF를 통한 broad한 개인화를 제공한다. 유저간의 유사도는 시청한 비디오들의 ID, search query token, demographics 등의 coarse한 feature들로 표현 될 수 있다. 최선의 추천을 나타내기 위해선 후보들 간의 상대적 중요성을 구별해서 보여줄 필요가 있다. 유저와 비디오들을 표현하는 풍부한 피쳐 셋들을 사용한 목적 함수를 따라서 각각의 비디오에 점수를 매김으로써, 랭킹을 매길 수 있다. 가장 높은 점수의 비디오들이 점수에 따라 유저에게 노출됨. 알고리즘 발전 과정에서는 precision, recall, ranking loss 등의 offline metrics들을 사용했으나, 최종적인 알고리즘의 효율 결정에 있어서는 실제 실험에서의 A/B 테스트를 사용했다. 미세한 CTR변동이나 시청 시간과 같은 다양한 측정들을 사용했는데, offline측정이 실제 A/B 테스트 결과와 항상 높은 상관관계가 있는 것이 아니므로 중요한 부분이라고 볼 수 있다.

<center><img src="https://imgur.com/CLl8V0q.png" width="60%" height="60%"></center>

## Candidate generation
유저의 이전 시청 정보가 임베딩 된 네트워크를 활용한 얕은 NN의 반복을 통해 기존의 MF 방법론을 모방하였음. 이러한 관점에서 보면 이 방법론은 non-linear generalization of factorization techniques으로 볼 수 있음.

## recommendation as Classification
유튜브 알고리즘에 explicit 피드백이 존재하긴 하지만(영상에 대한 좋아요, 서베이 등), 영상 시청에 대한 Implicit 피드백을 모델 트레이닝에 사용했음. 특정 영상을 끝까지 시청한 것은 긍정적 피드백의 일종으로 볼 수 있음.
유저와 문맥 쌍의 고차원 임베딩에 해당하는 벡터 $u$와, 각각의 후보 영상의 임베딩에 해당하는 $v$를 활용한 수식은 다음과 같음

<center><img src="https://imgur.com/qnatxUi.png" width="60%" height="60%"></center>

수 백만개의 영상들 중에 특정 $t$에서의 영상 $w_t$를 시청할 확률을 위와같이 표현하며 $$u\in\mathbb{R}^N$$ 는 유저, 문맥 pair의 고차원 임베딩을 나타낸다. $$v_{j}\in\mathbb{R}^N$$는 각 후보 영상들의 고차원 임베딩을 나타낸다.
## Model Architecture
유저의 영상 시청 히스토리는 임베딩을 통해 dense input으로 매핑 된다. 네트워크는 고정된 사이즈의 dense input을 필요로 하며, 이를 위해 단순하게 영상들의 임베딩들 간의 평균을 낸 값이 다른 방법들에 비해 가장 좋은 성능을 보여주었다 (sum, component-wise max 등보다 좋은 성능) 이렇게 정리된 feature들은 넓은 첫 레이어로 concatenate 됐다. 이 후에 ReLU 레이어들로 연결된다. Figure 3는 이에 대한 전체적인 설명이다.
>Feature 데이터를 정리하는 이 부분에 대해 조금 더 생각을 해보자. 우리가 100개의 영화를 각각 길이 50의 벡터로 표현 가능하게 임베딩을 했다면, 하나의 영화는 50x1 사이즈의 벡터로 표현이 가능할 것이다. 이렇게 영화 하나를 50행의 길이로 표현한 한 벡터를 아래의 figure에서 파란색 embedded video watches 벡터(수직으로 긴 줄) 하나로 표현한 듯 하다. 100개의 영화가 있다고 가정했으므로 이렇게 생긴 50x1 파란 벡터가 총 100개 있을 것이다. 해당 100개 벡터들을 모두 평균(average)를 내서 watch vector 인풋으로 사용하겠다는 것으로 이해된다.

<center><img src="https://imgur.com/SjEoNGA.png" width="60%" height="60%"></center>

이런식으로 MF(matrix factorization)의 일반화된 형태로 DNN을 사용하면, 임의의 연속 변수와 불연속 변수들이 모두 모델에 포함될 수 있다는 것이 장점이다. 사용자의 검색 히스토리 또한 영상 시청 히스토리와 비슷한 방식으로 처리한다. 유저의 검색 쿼리가 토큰화 돼서 사용된다. 유저의 지역과 장치들이 임베딩되고 concatenated된다. 사용자의 성별, 로그인 상태, 나이와 같은 단순한 feature들은 [0,1] 사이로 normalized된 이후 input으로 바로 포함된다.
## Label and Context Selection
 추천 시스템은 surrogate problem을 해결하는 것을 포함하곤 한다. 정확하게 평점에 대한 예측을 하는 것은 결국에 효율적인 영화 추천 시스템과 직결되는 문제이다.
 > 어떤 유저가 특정 영화에 대해 내릴 평점에 대해 우리가 정확하게 예측 할 수 있다면, 그냥 단지 예측 평점을 내림차순으로 정리해서 보여주는 것이 결국 추천시스템과 같다는 말이다.

 우리가 사용하는 트레이닝 데이터 셋은 우리가 만든 추천시스템에 기반해 사용자가 시청한 데이터 뿐만 아니라, 모든 유튜브 플랫폼의 시청 데이터를 포함하고 있다.
 > 다른 싸이트에서 유튜브로 링크를 걸거나, 우리가 추천하지 않은 영상 시청에 대한 데이터들도 모두 포함 돼있다.

만약 유튜브가 우리가 추천한 영상에 의해서만 시청이 이뤄지는 구조라면, 새로운 영상들이 시청자들에게 다가갈 수 없고, 추천이 너무 과하게 exploitation의 방향으로 흘러가게 될 것이다. 만약에 유저들이 우리가 제공하는 추천시스템이 아닌 다른 방법을 통해 영상을 발견한다면, 우리는 빠르게 이러한 발견을 CF를 통해 캐치해내길 바랄 것이다.
